%!TEX root = cl2-project.tex
\subsection{Word2Vec}

Most current NLP systems treat words as atomic units, i.e., words are represented as a vector of size $V$ (where $V$ is the size of the vocabulary), with the value 1 only at the index of that word and 0 at all other places. The problem with this representation is that there is no notion of similarity between words.

Recently, with the advent of deep learning models which can trained on huge datasets, there has been a lot of interest in the field of learning distributed word representations. The idea in distributed representations of words is to represent words as points in some space (usually a vector space) with the expectation that similar words will be close to each other in that vector space. Words can have multiple degrees of similarity, both syntactically and semantically. Using multidimensional vectors, it is possible to capture all these different degrees of similarity. Another advantage of using these representations in various models is, when model parameters are adjusted in response to a particular word or word sequence, the improvements will carry over to occurrences of similar words and sequences.

These models are able to capture rich semantic information and have been shown to significantly improve NLP applications such as sentiment analysis, POS tagging, NER, etc. In this work, we explored a specific model for learning distributed representations of words called the Skip Gram model. The software package using this model is popularly known as $word2vec$.

The Skip Gram model is a very simple model which can learn distributed representations of words from huge datasets. This model has gained a lot of popularity because of the syntactic and semantic relationships it captures, in the form of linear regularities. More specifically, one can show that the following relationships among the word vectors hold. \\
$vector('car') - vector('cars') \approx vector('chair') - vector('chairs')$ \\
$vector('king') - vector('man') \approx vector('queen') - vector('woman')$

In this work, we attempted to build vector representations of phrases, in this case, status updates for each user. The status updates for each user were concatenated together, and we learnt a vector representation for the text associated to each user. Thus, each user was associated with a vector representation belonging in the same vector space as the words.

We did this by using a very simple technique. Let's suppose we have learnt the individual word representations from some big dataset. We use the preprocessing steps mentioned previously on the statuses for each user. This removes all the stop words and leaves us with statuses contained only of relevant words. Then, for each user, we just do a simple average over all the vectors of the words in each status for that user. In spite of being such a simple technique, this does make intuitive sense. Users who are posting depressed status updates more often would get an average vector more close to the depressed region in the space. On the other hand, people who post positive status updates should lie in a more happier part of the vector space. Using this intuition, we used as features just the average vector representation associated with each user and ran a supervised classification task on it.

We have not yet talked about how we can train the individual word representations in our model. According to us, there are three main approaches one can take towards this while using the $word2vec$ tool. Pre-trained 300 dimensional word vectors are available online which can be used to find the vector representation for each word. These word representations were trained on a Google News corpus containing around a 100 billion words with a vocabulary size of 3 million. However, Google News corpus data is very different from the conversational style of Facebook status updates. Thus, just using these word representations won't give us a very good result. We demonstrate this in the next section, where we show poor classification results on just using these pre-trained vectors. The second approach one can use is to train the Skip Gram model on the unlabeled data set of Facebook status updates that we have been provided. However, this is not feasible because of the following reason. The Skip Gram model is a very simple 2 layer model. While this simplicity in the model helps make the model extremely fast compared to other word representation learning models, it also means that it needs a lot more data to train effectively. Data sets containing at least hundreds of millions of words are required to train the Skip Gram properly. This makes the unlabeled data set of the Facebook status updates unsuitable for training the model.

The final approach one can take, and the one we think has the most promise, is to initialize the Skip Gram model with the pre-trained word vector representations and then to continue training the model using the unlabeled data set of Facebook status updates. This should initialize the word vectors to a good position, and the appropriate domain changes should be captured while we continue training on the unlabeled status updates. The $word2vec$ package available on Google has no way of doing this. The gensim toolkit for word2vec has the capability of doing an online training as mentioned. However, it cannot do so with the pre-trained word vectors learnt from the Google News corpus since it lacks vital information like the vocabulary tree which is required for the gensim code. Thus, we would have to first train our model on the Google News corpus and then do the domain adaptation by continuing training on the unlabeled status updates. This requires large computing power and thus we were unable to pursue this idea.

